{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2831c67",
   "metadata": {},
   "source": [
    "# Capstone project: Loan Default\n",
    "# Name: Matt McCarron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984b2bd0",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6bad28",
   "metadata": {},
   "source": [
    "We'll start by importing the Loan default data and checking for any null variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7462a2d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoanID            0\n",
       "Age               0\n",
       "Income            0\n",
       "LoanAmount        0\n",
       "CreditScore       0\n",
       "MonthsEmployed    0\n",
       "NumCreditLines    0\n",
       "InterestRate      0\n",
       "LoanTerm          0\n",
       "DTIRatio          0\n",
       "Education         0\n",
       "EmploymentType    0\n",
       "MaritalStatus     0\n",
       "HasMortgage       0\n",
       "HasDependents     0\n",
       "LoanPurpose       0\n",
       "HasCoSigner       0\n",
       "Default           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Read in data\n",
    "loan = pd.read_csv(\"Loan_default.csv\")\n",
    "\n",
    "#checking for any null values\n",
    "pd.isnull(loan).sum()\n",
    "\n",
    "#There are no null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56ada0e",
   "metadata": {},
   "source": [
    "# Encode and Split data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2706959a",
   "metadata": {},
   "source": [
    "Below we split the data into its categorical and non-categorical counterparts. We encode the categorical data and then recombine. Additionally, we need to remove the Loan_ID from the dataset as it will not be used in model training. Lastly, we split data into feature and response data and create training and testing datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07b73b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#non-categorical variables\n",
    "non_cat = ['LoanID', 'Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', \n",
    "           'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio', 'Default']\n",
    "\n",
    "#categorical variables\n",
    "cat = ['Education', 'EmploymentType', 'MaritalStatus', 'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner']\n",
    "\n",
    "#split between categorical data and numeric\n",
    "non_cat_dat = loan[non_cat]\n",
    "cat_dat = loan[cat]\n",
    "\n",
    "\n",
    "#One hot encode data\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit(cat_dat)\n",
    "cat_dat_encoded = encoder.transform(cat_dat)\n",
    "cat_dat_encoded = pd.DataFrame(cat_dat_encoded.toarray(), columns=encoder.get_feature_names_out())\n",
    "\n",
    "#recombine\n",
    "loan_OHE = non_cat_dat.merge(cat_dat_encoded, left_index=True, right_index=True)\n",
    "\n",
    "#drop loan_id\n",
    "loan_OHE = loan_OHE.drop('LoanID', axis = 'columns')\n",
    "\n",
    "#split between feature and response data\n",
    "x_loan_OHE = loan_OHE.drop('Default', axis = 'columns')\n",
    "y_loan_OHE = loan_OHE['Default']\n",
    "\n",
    "#split data between a train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_loan_OHE, y_loan_OHE, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc6a279",
   "metadata": {},
   "source": [
    "# Random Forest Classifier Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15230a79",
   "metadata": {},
   "source": [
    "First, we are going to train a random forest classifier. We'll start performing grid searches for parameter selection and evaluate the model through cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da40fb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are:  {'max_depth': 32, 'min_samples_split': 2, 'n_estimators': 5}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Use F1 score as the scoring metric\n",
    "scorer = make_scorer(f1_score)\n",
    "\n",
    "# -----\n",
    "# Coarse-Grained RandomForestRegressor GridSearch\n",
    "# -----\n",
    "\n",
    "param_grid = {\"max_depth\":[4,8,16,32], \n",
    "              \"n_estimators\":[5,10,20,50], \n",
    "              \"min_samples_split\":[2,8,14,20]\n",
    "}\n",
    "\n",
    "gs_model = RandomForestClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=gs_model, param_grid=param_grid, scoring=scorer, cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"The best parameters are: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60534064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are:  {'max_depth': 24, 'min_samples_split': 2, 'n_estimators': 5}\n"
     ]
    }
   ],
   "source": [
    "# -----\n",
    "# Refined RandomForestRegressor GridSearch\n",
    "# -----\n",
    "\n",
    "param_grid = {\"max_depth\":[16,24,32], \n",
    "              \"n_estimators\":[5,6,7,8,9,10,11,12,13,14], \n",
    "              \"min_samples_split\":[2,3,4]\n",
    "}\n",
    "\n",
    "gs_model = RandomForestClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=gs_model, param_grid=param_grid,scoring=scorer, cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"The best parameters are: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "070ae47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are:  {'max_depth': 28, 'min_samples_split': 2, 'n_estimators': 1}\n"
     ]
    }
   ],
   "source": [
    "# -----\n",
    "# Final RandomForestRegressor GridSearch\n",
    "# -----\n",
    "\n",
    "param_grid = {\"max_depth\":[20,24,28], \n",
    "              \"n_estimators\":[1,2,3,4,5,6,7,8,9], \n",
    "              \"min_samples_split\":[2,3,4]\n",
    "}\n",
    "\n",
    "gs_model = RandomForestClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=gs_model, param_grid=param_grid,scoring=scorer, cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"The best parameters are: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a924230",
   "metadata": {},
   "source": [
    "The optimal model parameters for the `RandomForestClassifier` class are:\n",
    "\n",
    "- `max_depth = 28`\n",
    "- `n_estimators = 1`\n",
    "- `min_samples_split = 2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a9dbdf",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17c2669",
   "metadata": {},
   "source": [
    "Next, we are going to train a gradient boosting classifier. We'll start performing grid searches for parameter selection and evaluate the model through cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e2ccf69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are:  {'learning_rate': 0.75, 'max_depth': 32, 'n_estimators': 20}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# -----\n",
    "# Coarse-Grained GradientBoostingRegressor GridSearch\n",
    "# -----\n",
    "\n",
    "param_grid = {\"max_depth\":[4,8,16,32], \n",
    "              \"n_estimators\": [10,20,40,80], \n",
    "              \"learning_rate\":[.25, .5, .75]}\n",
    "\n",
    "gs_model = GradientBoostingClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=gs_model, param_grid=param_grid, scoring=scorer, cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"The best parameters are: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f224d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are:  {'learning_rate': 0.75, 'max_depth': 28, 'n_estimators': 20}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Use F1 score as the scoring metric\n",
    "scorer = make_scorer(f1_score)\n",
    "\n",
    "# -----\n",
    "# Refined GradientBoostingRegressor GridSearch\n",
    "# -----\n",
    "\n",
    "param_grid = {\"max_depth\":[24,28,32], \n",
    "              \"n_estimators\":[15,20,25,30], \n",
    "              \"learning_rate\":[.6,.75, .9]}\n",
    "\n",
    "gs_model = GradientBoostingClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=gs_model, param_grid=param_grid,scoring=scorer, cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"The best parameters are: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc8a8d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are:  {'learning_rate': 0.75, 'max_depth': 30, 'n_estimators': 20}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Use F1 score as the scoring metric\n",
    "scorer = make_scorer(f1_score)\n",
    "\n",
    "# -----\n",
    "# Final GradientBoostingRegressor GridSearch\n",
    "# -----\n",
    "\n",
    "param_grid = {\"max_depth\":[26,28,30], \n",
    "              \"n_estimators\":[17,20,23], \n",
    "              \"learning_rate\":[.7,.75, .8]}\n",
    "\n",
    "gs_model = GradientBoostingClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=gs_model, param_grid=param_grid,scoring=scorer, cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"The best parameters are: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6404ce",
   "metadata": {},
   "source": [
    "The optimal model parameters for the `GradientBoostingClassifier` class are:\n",
    "\n",
    "- `max_depth = 30`\n",
    "- `n_estimators = 20`\n",
    "- `learning_rate = .75`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a79edb8",
   "metadata": {},
   "source": [
    "# Random Forest Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2893686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=28, n_estimators=1, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=28, n_estimators=1, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=28, n_estimators=1, random_state=42)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Train random forest classifier based on parameters above\n",
    "rnd_clf = RandomForestClassifier(min_samples_split = 2, max_depth = 28, n_estimators = 1, random_state = 42)\n",
    "rnd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c08c11",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfb4b243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingClassifier(learning_rate=0.75, max_depth=30, n_estimators=20,\n",
       "                           random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(learning_rate=0.75, max_depth=30, n_estimators=20,\n",
       "                           random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingClassifier(learning_rate=0.75, max_depth=30, n_estimators=20,\n",
       "                           random_state=42)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#Train gradient boosting classifier based on parameters above\n",
    "gbrt = GradientBoostingClassifier(learning_rate = .75, max_depth = 30, n_estimators = 20, random_state = 42)\n",
    "gbrt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf3c7b3",
   "metadata": {},
   "source": [
    "# Figure Out Optimal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de3aa009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_bc523\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_bc523_level0_col0\" class=\"col_heading level0 col0\" >Models</th>\n",
       "      <th id=\"T_bc523_level0_col1\" class=\"col_heading level0 col1\" >Item</th>\n",
       "      <th id=\"T_bc523_level0_col2\" class=\"col_heading level0 col2\" >Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_bc523_row0_col0\" class=\"data row0 col0\" >Random Forest</td>\n",
       "      <td id=\"T_bc523_row0_col1\" class=\"data row0 col1\" >Accuracy</td>\n",
       "      <td id=\"T_bc523_row0_col2\" class=\"data row0 col2\" >0.807167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_bc523_row1_col0\" class=\"data row1 col0\" ></td>\n",
       "      <td id=\"T_bc523_row1_col1\" class=\"data row1 col1\" >Precision</td>\n",
       "      <td id=\"T_bc523_row1_col2\" class=\"data row1 col2\" >0.196494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_bc523_row2_col0\" class=\"data row2 col0\" ></td>\n",
       "      <td id=\"T_bc523_row2_col1\" class=\"data row2 col1\" >Recall</td>\n",
       "      <td id=\"T_bc523_row2_col2\" class=\"data row2 col2\" >0.216610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_bc523_row3_col0\" class=\"data row3 col0\" ></td>\n",
       "      <td id=\"T_bc523_row3_col1\" class=\"data row3 col1\" >F1 score</td>\n",
       "      <td id=\"T_bc523_row3_col2\" class=\"data row3 col2\" >0.206063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_bc523_row4_col0\" class=\"data row4 col0\" ></td>\n",
       "      <td id=\"T_bc523_row4_col1\" class=\"data row4 col1\" >AUC curve</td>\n",
       "      <td id=\"T_bc523_row4_col2\" class=\"data row4 col2\" >0.550457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_bc523_row5_col0\" class=\"data row5 col0\" ></td>\n",
       "      <td id=\"T_bc523_row5_col1\" class=\"data row5 col1\" >Log Loss</td>\n",
       "      <td id=\"T_bc523_row5_col2\" class=\"data row5 col2\" >6.947860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_bc523_row6_col0\" class=\"data row6 col0\" >Gradient Boosting</td>\n",
       "      <td id=\"T_bc523_row6_col1\" class=\"data row6 col1\" >Accuracy</td>\n",
       "      <td id=\"T_bc523_row6_col2\" class=\"data row6 col2\" >0.819013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_bc523_row7_col0\" class=\"data row7 col0\" ></td>\n",
       "      <td id=\"T_bc523_row7_col1\" class=\"data row7 col1\" >Precision</td>\n",
       "      <td id=\"T_bc523_row7_col2\" class=\"data row7 col2\" >0.221091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_bc523_row8_col0\" class=\"data row8 col0\" ></td>\n",
       "      <td id=\"T_bc523_row8_col1\" class=\"data row8 col1\" >Recall</td>\n",
       "      <td id=\"T_bc523_row8_col2\" class=\"data row8 col2\" >0.224576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_bc523_row9_col0\" class=\"data row9 col0\" ></td>\n",
       "      <td id=\"T_bc523_row9_col1\" class=\"data row9 col1\" >F1 score</td>\n",
       "      <td id=\"T_bc523_row9_col2\" class=\"data row9 col2\" >0.222820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_bc523_row10_col0\" class=\"data row10 col0\" ></td>\n",
       "      <td id=\"T_bc523_row10_col1\" class=\"data row10 col1\" >AUC curve</td>\n",
       "      <td id=\"T_bc523_row10_col2\" class=\"data row10 col2\" >0.560617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_bc523_row11_col0\" class=\"data row11 col0\" ></td>\n",
       "      <td id=\"T_bc523_row11_col1\" class=\"data row11 col1\" >Log Loss</td>\n",
       "      <td id=\"T_bc523_row11_col2\" class=\"data row11 col2\" >2.195316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x189e3e91760>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "#Use testing data\n",
    "y_pred_rnd = rnd_clf.predict(X_test)\n",
    "y_pred_grd = gbrt.predict(X_test)\n",
    "\n",
    "#Measurement scores for Random Forest\n",
    "acc_score_rnd = accuracy_score(y_test, y_pred_rnd)\n",
    "prec_score_rnd = precision_score(y_test, y_pred_rnd)\n",
    "recall_score_rnd = recall_score(y_test, y_pred_rnd)\n",
    "cm_rnd = confusion_matrix(y_test, y_pred_rnd)\n",
    "f1_rnd = f1_score(y_test, y_pred_rnd)\n",
    "# Calculate ROC curve\n",
    "fpr_rnd, tpr_rnd, thresholds_rnd = roc_curve(y_test, y_pred_rnd)\n",
    "# Calculate AUC\n",
    "roc_auc_rnd = auc(fpr_rnd, tpr_rnd)\n",
    "probabilities_rnd = rnd_clf.predict_proba(X_test)\n",
    "logloss_rnd = log_loss(y_test, probabilities_rnd)\n",
    "\n",
    "#Measurement scores for Gradient Booster\n",
    "acc_score_grd = accuracy_score(y_test, y_pred_grd)\n",
    "prec_score_grd = precision_score(y_test, y_pred_grd)\n",
    "recall_score_grd = recall_score(y_test, y_pred_grd)\n",
    "cm_grd = confusion_matrix(y_test, y_pred_grd)\n",
    "f1_grd = f1_score(y_test, y_pred_grd)\n",
    "# Calculate ROC curve\n",
    "fpr_grd, tpr_grd, thresholds_grd = roc_curve(y_test, y_pred_grd)\n",
    "# Calculate AUC\n",
    "roc_auc_grd = auc(fpr_grd, tpr_grd)\n",
    "probabilities_grd = gbrt.predict_proba(X_test)\n",
    "logloss_grd = log_loss(y_test, probabilities_grd)\n",
    "\n",
    "#Exhibit to compare both models\n",
    "exhibit = [['Random Forest', 'Accuracy', acc_score_rnd],\n",
    "                ['', 'Precision', prec_score_rnd],\n",
    "                ['', 'Recall', recall_score_rnd],\n",
    "                 ['', 'F1 score', f1_rnd],\n",
    "                 ['', 'AUC curve', roc_auc_rnd],\n",
    "                 ['', 'Log Loss', logloss_rnd],\n",
    "                ['Gradient Boosting', 'Accuracy', acc_score_grd],\n",
    "                ['', 'Precision', prec_score_grd],\n",
    "                ['', 'Recall', recall_score_grd],\n",
    "                ['', 'F1 score', f1_grd],\n",
    "                 ['', 'AUC curve', roc_auc_grd],\n",
    "                ['', 'Log Loss', logloss_grd]]\n",
    "\n",
    "exhibit = pd.DataFrame(exhibit, columns = ['Models', 'Item', 'Score'])\n",
    "\n",
    "exhibit.style.hide(axis='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba0e098",
   "metadata": {},
   "source": [
    "In the above exhibit, we compare our Random Forest and Gradient Boosting model across a number of different metrics. While we optimized our model based on F1 score, it's important to compare among a number of different metrics in order to get a more comprehensive look of how our models are working.\n",
    "\n",
    "We chose F1 score as our scoring metric because it takes into account both precision and recall. Ultimately, default data is imbalanced where around 88% don't default vs. 12% defaulting. Because of this, accuracy alone is not a reliable metric. Additionally, loan defaulting does not have the need to favor precision or recall over the other. Therefore, we scored our grid searches on F1, but it will be important to review all of the above metrics. \n",
    "\n",
    "Based on the metrics above, we will move forward with the gradient boosting model. The models are roughly similar, but the gradient boosting model edges out the random forest model in all metrics, so we'll move forward with that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a7f622a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF confusion matrix\n",
      "[[39944  5226]\n",
      " [ 4622  1278]]\n",
      "GB confusion matrix\n",
      "[[40502  4668]\n",
      " [ 4575  1325]]\n"
     ]
    }
   ],
   "source": [
    "#Confusion matrix\n",
    "print(\"RF confusion matrix\")\n",
    "print(cm_rnd)\n",
    "print(\"GB confusion matrix\")\n",
    "print(cm_grd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec5d965",
   "metadata": {},
   "source": [
    "# Save model for deployment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3069613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#Utilize pickle to save model down to use in flask\n",
    "with open(\"GradBoost_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(gbrt, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a512ada",
   "metadata": {},
   "source": [
    "# Breakeven Financial Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b887bb9b",
   "metadata": {},
   "source": [
    "I will utilize the gradient boosting model to extract the probability of not defaulting. I will then utilize the below formula to calculate the expected profit from the loan. The below model calculates the expected profit by calculating the profit from interest if they do not default multiplied by the probability of not defaulting minus the expected loss experienced if they do default multiplied by that likelihood (1-probability).\n",
    "\n",
    "Profit = Probability x LoanAmount x LoanTerm / 12 x InterestRate / 100 - (1-Probability) x LoanAmount\n",
    "\n",
    "The function that calculates this and is utilized in the model is written below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "893339df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loan(probability, LoanAmount, LoanTerm, InterestRate):\n",
    "    profit = probability*LoanAmount*LoanTerm/12*InterestRate/100 - (1-probability)*LoanAmount\n",
    "    if profit > 0:\n",
    "        status = 'Approved'\n",
    "    else:\n",
    "        status = 'Denied'\n",
    "    return status"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
